to do

0.7
(4873 sloc)

---

are meta-statements really worth it?
consider eval/exec
eval has valid uses for namespace tricks, string manipulation, et cetera
exec is kind of dangerous and absolutely ruins the type system

make meta-statements occupy their own scope; cannot affect external scope except by messaging
compensate by adding apply
wow, sophia is surprisingly functional, huh?

---

change register allocation so that constants are assigned negative numbers / hash registers / figure out a way to give every register a unique hash

---

anonymous routines
(num x, num y => x + y)
(awaits x => x + 1)
(extends x => x = 1)
the return type of an anonymous function is inferred from its last operation

make single-line function definitions into expressions that return the function
this is going to require some specialised parsing

---

use statement
analogous to import statements or rust use statements
sophia needs a mechanism for extending namespaces regardless of how useful linking is

the use statement extends the current namespace with all top-level routine definitions in the target file
files targeted by the use statement are *not* executed, only compiled
code other than routine definitions is completely ignored
this solves the circular import problem by simply. not executing use statements
sophia uses the namespace at the time of execution, not definition, so just put the use statement inside the function
or just put the use statement in the calling environment! not always an optimal solution but it still works

---

iteration over streams
the for statement just gets the next element from any given iterable
it stands to reason that this could work for infinite streams too

---

table type
implements a mutable collection of records with typed fields
take from sql, cobb's operators for table querying and manipulation

not using oop is all very well and good, but sophia lacks for useful complex data structures
custom tables should resolve this issue quite nicely
combined with the distributed functionalities of sophia, some powerful rdms capabilities are possible

tables should be "mutable" only in well-defined operations (insertion, deletion), like any other sequence
table operations and message passing should return copy, not reference
need to adhere to acid principles
tables should behave like bona-fide 2-dimensional structures and be appropriately easy to access

---

plans

---

so what's the deal with sophia's type system?

subtyping doesn't really work with sophia's current design philosophy and is extremely difficult to work with
subtyping is just adding new constraints onto an existing type
therefore, it is explicitly equivalent to the intersection of the type and its supertype

instead, we might consider: abstract data types
types are defined by a constrained set of values and a set of operations upon those values
a constraint can be any boolean predicate
for example: values that are integers; values with a length of 3; values whose elements are all integers

types are described as pointers to a unique base concrete data type and a collection of constraints
pointers endure for the lifetime of the descriptor, allowing types to be referenced wherever needed
for instance:

integer
	base: <number>
	integer: <constraint>

their operations are defined as functions, as usual

type notation becomes unnecessary because this functionality can be replicated by composition and aliasing
for instance:

a is even | integer
a
	base: <number>
	even: <constraint>
	integer: <constraint>

this format also vastly simplifies the type operators

the semantics of the 'extends' keyword changes:
	the extended type defines the base data type
	the extended type is the type of the checked value
	the defined type takes the properties of the extended type
for instance:

type integer extends number => integer % 1 = 0

integer
	base: <number>
	integer: <integer>

type even extends integer => even % 2 = 0

even
	base: <number>
	integer: <integer>
	even: <even>

type intersections take the union of their properties
type unions take the intersection of their properties
intersections with different data types are invalid
unions infer a single data type, otherwise a lot of things become extremely difficult very quickly
yes, your int | str is all very well and good until dispatch finds multiple valid methods and everything falls apart
in fact, under a strict definition where unions take the intersection of properties:
unions don't actually have to have any data type at all
very cool new property of structural typing: the union of two types is also their mutual supertype

structural typing, descriptor-based types, and multiple dispatch allow us to do some really neat things
for instance:
a is a[b] // Specify element type
a is a[3] // Specify length

---

all very good, barring implementation details
new problem. what does multiple dispatch mean in this type system?

dispatch has a conceptual notion of matching the most specific possible method for a type
with subtyping, it was easy to determine that specificity related to subtype

dispatch should first attempt to match the data type, obviously
it should then match the method that fulfils the most properties of the type without over-specifying
properties are just boolean predicates, so fulfilling one is a binary state

here's a fun problem
dispatch must be unambiguous and deterministic
dispatch can match the method with the most matching properties, sure -
but what happens when two different signatures have the same number of matches?
in what order do properties take precedence?
oh no

a descriptor is able to have arbitrarily many properties
these properties can be user-defined
there exists no natural hierarchy for these properties
some other form of precedence has to be used

---

never mind! recency precedence is a nightmare to implement
there has got to be some better way to establish precedence

alternative proposal: properties take precedence in order of definition
this makes sense, right?
more elemental and essential properties of a type are defined first
types that extend other types can be considered as analogous to subtypes

for instance, compare these two signatures:

a
	base: list
	sequence: null
	length: 3

b
	base: list
	sequence: <number>
	length: null

if we enter a value with this type signature:

value
	base: list
	sequence: <number>
	length: 3

a and b are both valid candidates for dispatch

under type precedence, dispatch selects b

---

what's the best dispatch algorithm for this type framework?
a fast algorithm needs to take advantage of distinct characteristics of the descriptor

properties are binary
descriptors only contain the properties they actually possess
valid candidates specify as many of the value's properties as possible
valid candidates specify no properties that the value does not have

it makes sense for dispatch to distinguish only significant criteria, as before
this is a little more difficult, though
the previous tree was well-ordered for precedence
this tree *is* well-ordered but the mechanism to do so is a little more difficult

traversal and verification ensure that dispatch cannot select an invalid candidate
tree traversal ensures a maximum of 1 valid candidate
so. how do you ensure that the *correct* valid candidate is selected?
a naïve approach produces an opaque precedence

---

...actually, isn't using pointers desirable?
consider this:

type even extends number => even % 2 = 0

a is even | integer

type even extends string => length(even) % 2 = 0

b is even | string

if your references are strings, both a and b will use the 2nd definition
if your references are pointers, however, this becomes absolutely fine
this also massively reduces namespace lookups

---

remind me how dispatch works, again?

dispatch tree
methods added to tree using their most significant distinguishing property
branch true if index in range of arity and if the distinguishing property is in the type descriptor
if a value is specified, the descriptor also needs to match the value exactly

---

dispatch format

pointers? pointers???
if you're really going for structural typing then you don't need the names, do you?
you only need an ordered list of methods
and the special attributes for property values, of course

functions with a null return type commonly denote functions that do not return
so it makes sense for a null final typedef to indicate no return value -
or to automatically fill address 0 with null

empty final typedefs indicate that the return value should be inferred

---

new routine formats
dispatch merged into multimethod, descriptor merged into typedef
all routines are now anonymous by default
routines can get their name during calling by checking the name of the current instruction

...hey, how do you generate instructions for a recursive anonymous function?
actually, this straight-up doesn't matter
the function gets bound to its own namespace as an implicit argument to itself
within the function, you can refer to the name it was declared with without issue
this works in a similar way to how you use the name of a type to refer to its checked value
e.g:
type even extends int => even % 2 = 0
int add (int n) => add(n + 1)

consider the use of a current environment variable

---

dispatch. what?

naïve dispatch requires n*m passes to compare signatures
we can do better than that, but god it's going to hurt to implement

extend:
traverse tree to closest matching node
first match for arity
then match for type
use the first type that the new method has that the existing one doesn't
if there are none, try again the other way round
if neither of these work, the signatures must be identical, so overwrite

multimethod format
	true: path if true
	false: path if false
	property: a trinary function taking a type signature, value, and index 
	value: usually a type method or an integer
	index: the index for the item in the type signature being tested

special properties must be present in types and properties of a typedef
make a universal check that checks the presence of a property and checks against its value

---

type properties are presenting another problem
keeping them well-encapsulated makes them very hard to expose
the only way to find a sequence's length is to search all of its type properties to see if its length is defined and then get the length from the property
that's fine, though
the real kicker is that not all sequences *have* a defined length
sure, sequence literals are defined with element and length
but methods that return sequences have statically defined return types can't guarantee these properties

to guarantee correct functionality, all data must have all possible properties defined for them
if you want to call length() on any sequence, which is extremely reasonable,
then you... need to know its length in advance?
that doesn't seem right

there's a semantic misunderstanding here
the typedef of a value does *not* describe every property that it has
it describes the properties that the user has guaranteed for it
if the user is unable to guarantee a given property for a type, they don't get to use it
dispatch depends on the properties it knows its arguments have
it does *not* attempt to downcast any arguments

for example:

[1, 2, 3]
> list.element:int.length:3

a: [1, 2, 3]
> list.element:int.length:3
user has allowed the dynamic type inference to determine a's properties

list a: [1, 2, 3]
> list
user has explicitly chosen not to guarantee properties; a could be reassigned any list

type triple: list[int][3]
triple a: [1, 2, 3]
> list.element:int.length:3
user has explicitly chosen to guarantee properties

in sophia, the user just has to do some more work to explicitly describe the properties of their data

side note: is it useful to retain all of a name's properties when reassigned?
for instance, in the above example, is a now restricted to the type list.element:int.length:3?
change the behaviour of untyped assignment so that it binds the type of the assigned value no matter what
for example:

a: [1, 2, 3]
> list.element:int.length:3
a: 1
> integer

this matches the behaviour of assignment in dynamic languages
which is what the user will be expecting when using untyped assignment

so this also works?

int a: 1
> integer
a: '1'
> string.element:string.length:1

i mean, i guess
it's not that hard to write the type explicitly anyway
since the user must alias any given type before it can be used in assignment
better not to make assignment context-dependent

---

operations that return to address 0

typed names in assertions
continue
break
return

basically, we're safe to assume that setting assertions to return to 0 will work

assertions what?

aghhh

assertions are a natural consequence of a LBYL philosophy
equivalent to an if statement but for some/none, and also catches errors

result types?

what are assertions used for?

type checking
	<type>(value) now returns boolean; redundant
some/none branching
	use the safety operator
catching unbound names
	don't reference unbound names, idiot
bounds checking
	use length types
	instead of going a[2] without knowing if that index is valid
	go list[2](a) to guarantee it
dispatch checking
	signature(<routine>) returns boolean; redundant
catching exceptions
	bad practice; exceptions aren't meant to be caught
	exceptions only kill the task anyway

assertions are unnecessary

---

new error model for sophia

functions can either return successfully or fail to execute
a failed function returns null
null is the value that represents that an exception occurred

if a single failure value is insufficient to represent your failure state,
that's your problem, not the language's
this problem can be solved with proper separation of concerns
any given function should only fail in 1 expected way

for instance, division can return null, but only on division by 0
therefore, the user always knows what exception has occurred

function signatures do not have to declare that they are able to return null
any function is expected to be able to return null

errors represent unexpected, unrecoverable problems
errors terminate their task, but *not* the program as a whole
this allows you to use a supervisor to restart a failed task, should you so choose
like in erlang, tasks are allowed to fail, and you should account for this
errors are severe faults, which incentivises LBYL

---

250047 function calls in 0.332 seconds

Ordered by: cumulative time

ncalls  tottime  percall  cumtime  percall filename:lineno(function)
	1    0.082    0.082    0.332    0.332 task.py:58(run)
10001    0.056    0.000    0.165    0.000 aletheia.py:382(__call__)			necessary
20000    0.039    0.000    0.059    0.000 mathos.py:190(__add__)			optimised
20001    0.045    0.000    0.059    0.000 aletheia.py:218(__gt__)			necessary without caching
10001    0.008    0.000    0.058    0.000 task.py:328(intern_next)			fine
10001    0.005    0.000    0.050    0.000 {built-in method builtins.next}	fine
10001    0.015    0.000    0.045    0.000 mathos.py:442(__iter__)			fine
10000    0.007    0.000    0.042    0.000 operators.py:10(b_add)			fine
20002    0.012    0.000    0.021    0.000 mathos.py:56(__new__)				necessary
10001    0.014    0.000    0.014    0.000 task.py:155(intern_bind)			necessary
60000    0.014    0.000    0.014    0.000 aletheia.py:30(__eq__)			necessary without caching
10000    0.012    0.000    0.012    0.000 task.py:297(intern_loop)			necessary
20002    0.008    0.000    0.008    0.000 {built-in method __new__ of type object at 0x00007FF8F7CF13D0} necessary
10001    0.007    0.000    0.007    0.000 mathos.py:399(__le__)				necessary without caching
10000    0.004    0.000    0.004    0.000 aletheia.py:513(check)			necessary without caching
10000    0.002    0.000    0.002    0.000 aletheia.py:412(__bool__)			necessary without caching
10001    0.002    0.000    0.002    0.000 aletheia.py:339(__bool__)			necessary without caching

120000 function calls (half of all calls in the runtime!) can be removed by well-implemented caching
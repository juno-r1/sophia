to do

0.7
(4873 sloc)

---

are meta-statements really worth it?
consider eval/exec
eval has valid uses for namespace tricks, string manipulation, et cetera
exec is kind of dangerous and absolutely ruins the type system

make meta-statements occupy their own scope; cannot affect external scope except by messaging
compensate by adding apply
wow, sophia is surprisingly functional, huh?

---

change register allocation so that constants are assigned negative numbers / hash registers / figure out a way to give every register a unique hash

---

anonymous routines
(num x, num y => x + y)
(awaits x => x + 1)
(extends x => x = 1)
the return type of an anonymous function is inferred from its last operation

make single-line function definitions into expressions that return the function
this is going to require some specialised parsing

---

use statement
analogous to import statements or rust use statements
sophia needs a mechanism for extending namespaces regardless of how useful linking is

the use statement extends the current namespace with all top-level routine definitions in the target file
files targeted by the use statement are *not* executed, only compiled
code other than routine definitions is completely ignored
this solves the circular import problem by simply. not executing use statements
sophia uses the namespace at the time of execution, not definition, so just put the use statement inside the function
or just put the use statement in the calling environment! not always an optimal solution but it still works

---

iteration over streams
the for statement just gets the next element from any given iterable
it stands to reason that this could work for infinite streams too

---

table type
implements a mutable collection of records with typed fields
take from sql, cobb's operators for table querying and manipulation

not using oop is all very well and good, but sophia lacks for useful complex data structures
custom tables should resolve this issue quite nicely
combined with the distributed functionalities of sophia, some powerful rdms capabilities are possible

tables should be "mutable" only in well-defined operations (insertion, deletion), like any other sequence
table operations and message passing should return copy, not reference
need to adhere to acid principles
tables should behave like bona-fide 2-dimensional structures and be appropriately easy to access

---

extended static checker for sophia

type checking

this is a pretty much a solved problem for built-in types and operations
this is also a solved problem for user-defined operations, since those are required to specify their pre-conditions and post-conditions
when you introduce user-defined types, however, this becomes extremely non-trivial

this would be fine if you were able to, say, define all type relations in header files
however, types can and should be defined anywhere
these are the tradeoffs that a dynamic language must make
in a sufficiently dynamic language, such as this one, not all properties of the program can be statically known

so what can we know?

only the internal scope of any given procedure can be known
a procedure can and will be called in any environment
per the language specification, a procedure's types *do* need to be known when it is defined
this does not, guarantee, however, that they will be known in the calling scope
if the types aren't known in the calling scope, the checker should invalidate the call, since it is guaranteed to fail

the checker cannot know anything about a type that is outside the scope it was defined
nor can it know anything about types created, for example, in meta-statements
if the checker encounters the name of a type that is bound but not defined in scope, it must assume that the type exists but cannot assume anything about it

necessary to deal with conditional execution
keep track of scope through blocks, treating them as their own scopes
it should be possible to type check completely linearly, progressing through blocks as they come

what can the checker do?

the checker serves to verify the pre-conditions and post-conditions of every operation
when it does this, it is able to guarantee certain properties of arguments
to this end, the checker is also capable of detecting when a type check is unnecessary

the checker should therefore be able to edit and remove unnecessary type checks

---

constant folding and propagation

as a virtual register machine, sophia has to create every register it needs - and populate them with constants - at compile time
therefore, every constant is known at compile time
all built-ins are also available at compile time
this allows you to evaluate a wide range of expressions at compile time

constant folding does not involve block statements or function calls
this is not just running the program

the checker should remove unnecessary instructions after performing constant folding

every constant is unique (apart from null), which means there's no problem deleting or overwriting constant registers that aren't &0

---

pre-evaluated dispatch

if you're going to be type checking the whole program
and doing so basically requires performing dispatch on every instruction to verify that it works
and dispatch is deterministic for intraprocedural data
then you've basically already pre-evaluated dispatch

this allows us to partially build the instruction cache at compile time

---

metis

forget everything that was written here before
just write the simplest static analyser possible
no scopes, no conditional branching, no routine operators
just bind the damn addresses

the namespace framework has to operate differently here so as to preserve the registers for runtime
keep track of types, no values

basic binding is complete; most operations now work
weirdly, conditional expressions actually work with the naive approach because they only bind to unnamed registers

there needs to be some way of encoding the current state of a program
states should point to the state directly above them
states should work for both compound statements and routines

---

binding implemented as normal

some internal instructions have a return type that must be inferred from the value (dependent type?)
this is a problem for metis, which does not have access to such a value
in these cases, what is the most consistent behaviour for metis?

metis should be conservative in its analysis
if a function's return type cannot be determined:
there should exist a signal that forces any further instruction reading from that return register to fail to cache

if at any point you don't know the return type of a function, how do you stop that from invalidating the rest of the program?
as it is, extremely common operations can cause the state of a whole program to become unknown

in practice, not so much
basically all data ends up either being bound, which always resolves the type, or being sent outside the routine
it's probably fine

loops do not matter, mostly
the continue and break keywords mean that loops have multiple end points
they should non-destructively force a resolve of all state in the loop up to that point, somehow
break also creates a problem in that it causes multiple states upon entry into the else statement
the entry to an else statement has the state that is the resolution of every break state

flag when a state continues or breaks and then pick it up when the loop state resolves

unfortunately, loops do matter
each possible end state of the loop needs to be fed back into the beginning of the loop
continue states and the default state go into the beginning of the loop and the end
break states go into the else statement or the end

---

problem detected

for conditional branches, metis currently assumes the mutual supertype of differently typed names
this presents a problem because it isn't the actual type of the name
this can cause incorrect method caching
metis needs to maintain the union type of the name
then, only if it is able to retrieve the same method for every type, it can cache

...or a more conservative approach can be taken
if metis finds that a name does not maintain the same type across all branches,
it can invalidate the cache

loop checking can still be done linearly
continues are converted into early LOOP labels, making them identical to the end of an actual loop
breaks are the BREAK label

---

operations that cause this state:

receive (n_rcv)
return type is determined by sending routine

resolve (u_rsv)
return type is determined by sending routine

binary safety operators (b_sfe)
return type is the type of the non-null operand

unsafety operator (u_usf)
return type is either the type of the operand or null

prototype constructor (u_new)
return type is the operand by definition

iterator control (.next)
return type is the member type of the iterator or null

skip (SKIP)
return type is the return type of the first function in the composition
actually, it can just be the type of the parameter it's chaining into
this is just a fancy bind-and-branch

cast (cast)
return type is the return type of the operand or null

reduce (reduce)
return type is the return type of the reducing function

consider rolling .next and .unloop into a single function

---

so what's the deal with sophia's type system?

subtyping doesn't really work with sophia's current design philosophy and is extremely difficult to work with
subtyping is just adding new constraints onto an existing type
therefore, it is explicitly equivalent to the intersection of the type and its supertype

instead, we might consider: abstract data types
types are defined by a constrained set of values and a set of operations upon those values
a constraint can be any boolean predicate
for example: values that are integers; values with a length of 3; values whose elements are all integers

types are described as pointers to a unique base concrete data type and a collection of constraints
pointers endure for the lifetime of the descriptor, allowing types to be referenced wherever needed
for instance:

integer
	base: <number>
	integer: <constraint>

their operations are defined as functions, as usual

type notation becomes unnecessary because this functionality can be replicated by composition and aliasing
for instance:

a is even | integer
a
	base: <number>
	even: <constraint>
	integer: <constraint>

this format also vastly simplifies the type operators

the semantics of the 'extends' keyword changes:
	the extended type defines the base data type
	the extended type is the type of the checked value
	the defined type takes the properties of the extended type
for instance:

type integer extends number => integer % 1 = 0

integer
	base: <number>
	integer: <integer>

type even extends integer => even % 2 = 0

even
	base: <number>
	integer: <integer>
	even: <even>

type intersections take the union of their properties
type unions take the intersection of their properties
intersections with different data types are invalid
unions infer a single data type, otherwise a lot of things become extremely difficult very quickly
yes, your int | str is all very well and good until dispatch finds multiple valid methods and everything falls apart
in fact, under a strict definition where unions take the intersection of properties:
unions don't actually have to have any data type at all
very cool new property of structural typing: the union of two types is also their mutual supertype

structural typing, descriptor-based types, and multiple dispatch allow us to do some really neat things
for instance:
a is a[b] // Specify element type
a is a[3] // Specify length

---

all very good, barring implementation details
new problem. what does multiple dispatch mean in this type system?

dispatch has a conceptual notion of matching the most specific possible method for a type
with subtyping, it was easy to determine that specificity related to subtype

dispatch should first attempt to match the data type, obviously
it should then match the method that fulfils the most properties of the type without over-specifying
properties are just boolean predicates, so fulfilling one is a binary state

here's a fun problem
dispatch must be unambiguous and deterministic
dispatch can match the method with the most matching properties, sure -
but what happens when two different signatures have the same number of matches?
in what order do properties take precedence?
oh no

a descriptor is able to have arbitrarily many properties
these properties can be user-defined
there exists no natural hierarchy for these properties
some other form of precedence has to be used

---

never mind! recency precedence is a nightmare to implement
there has got to be some better way to establish precedence

alternative proposal: properties take precedence in order of definition
this makes sense, right?
more elemental and essential properties of a type are defined first
types that extend other types can be considered as analogous to subtypes

for instance, compare these two signatures:

a
	base: list
	sequence: null
	length: 3

b
	base: list
	sequence: <number>
	length: null

if we enter a value with this type signature:

value
	base: list
	sequence: <number>
	length: 3

a and b are both valid candidates for dispatch

under type precedence, dispatch selects b

---

what's the best dispatch algorithm for this type framework?
a fast algorithm needs to take advantage of distinct characteristics of the descriptor

properties are binary
descriptors only contain the properties they actually possess
valid candidates specify as many of the value's properties as possible
valid candidates specify no properties that the value does not have

it makes sense for dispatch to distinguish only significant criteria, as before
this is a little more difficult, though
the previous tree was well-ordered for precedence
this tree *is* well-ordered but the mechanism to do so is a little more difficult

traversal and verification ensure that dispatch cannot select an invalid candidate
tree traversal ensures a maximum of 1 valid candidate
so. how do you ensure that the *correct* valid candidate is selected?
a naïve approach produces an opaque precedence

---

...actually, isn't using pointers desirable?
consider this:

type even extends number => even % 2 = 0

a is even | integer

type even extends string => length(even) % 2 = 0

b is even | string

if your references are strings, both a and b will use the 2nd definition
if your references are pointers, however, this becomes absolutely fine
this also massively reduces namespace lookups

---

remind me how dispatch works, again?

dispatch tree
methods added to tree using their most significant distinguishing property
branch true if index in range of arity and if the distinguishing property is in the type descriptor
if a value is specified, the descriptor also needs to match the value exactly

---

dispatch format

pointers? pointers???
if you're really going for structural typing then you don't need the names, do you?
you only need an ordered list of methods
and the special attributes for property values, of course

functions with a null return type commonly denote functions that do not return
so it makes sense for a null final typedef to indicate no return value -
or to automatically fill address 0 with null

empty final typedefs indicate that the return value should be inferred

---

new routine formats
dispatch merged into multimethod, descriptor merged into typedef
all routines are now anonymous by default
routines can get their name during calling by checking the name of the current instruction

...hey, how do you generate instructions for a recursive anonymous function?
actually, this straight-up doesn't matter
the function gets bound to its own namespace as an implicit argument to itself
within the function, you can refer to the name it was declared with without issue
this works in a similar way to how you use the name of a type to refer to its checked value
e.g:
type even extends int => even % 2 = 0
int add (int n) => add(n + 1)

consider the use of a current environment variable

---

dispatch. what?

naïve dispatch requires n*m passes to compare signatures
we can do better than that, but god it's going to hurt to implement

extend:
traverse tree to closest matching node
first match for arity
then match for type
use the first type that the new method has that the existing one doesn't
if there are none, try again the other way round
if neither of these work, the signatures must be identical, so overwrite

multimethod format
	true: path if true
	false: path if false
	property: a trinary function taking a type signature, value, and index 
	value: usually a type method or an integer
	index: the index for the item in the type signature being tested

special properties must be present in types and properties of a typedef
make a universal check that checks the presence of a property and checks against its value

---

type properties are presenting another problem
keeping them well-encapsulated makes them very hard to expose
the only way to find a sequence's length is to search all of its type properties to see if its length is defined and then get the length from the property
that's fine, though
the real kicker is that not all sequences *have* a defined length
sure, sequence literals are defined with element and length
but methods that return sequences have statically defined return types can't guarantee these properties

to guarantee correct functionality, all data must have all possible properties defined for them
if you want to call length() on any sequence, which is extremely reasonable,
then you... need to know its length in advance?
that doesn't seem right

there's a semantic misunderstanding here
the typedef of a value does *not* describe every property that it has
it describes the properties that the user has guaranteed for it
if the user is unable to guarantee a given property for a type, they don't get to use it
dispatch depends on the properties it knows its arguments have
it does *not* attempt to downcast any arguments

for example:

[1, 2, 3]
> list.element:int.length:3

a: [1, 2, 3]
> list.element:int.length:3
user has allowed the dynamic type inference to determine a's properties

list a: [1, 2, 3]
> list
user has explicitly chosen not to guarantee properties; a could be reassigned any list

type triple: list[int][3]
triple a: [1, 2, 3]
> list.element:int.length:3
user has explicitly chosen to guarantee properties

in sophia, the user just has to do some more work to explicitly describe the properties of their data

side note: is it useful to retain all of a name's properties when reassigned?
for instance, in the above example, is a now restricted to the type list.element:int.length:3?
change the behaviour of untyped assignment so that it binds the type of the assigned value no matter what
for example:

a: [1, 2, 3]
> list.element:int.length:3
a: 1
> integer

this matches the behaviour of assignment in dynamic languages
which is what the user will be expecting when using untyped assignment

so this also works?

int a: 1
> integer
a: '1'
> string.element:string.length:1

i mean, i guess
it's not that hard to write the type explicitly anyway
since the user must alias any given type before it can be used in assignment
better not to make assignment context-dependent

---

operations that return to address 0

typed names in assertions
continue
break
return

basically, we're safe to assume that setting assertions to return to 0 will work

assertions what?

aghhh

assertions are a natural consequence of a LBYL philosophy
equivalent to an if statement but for some/none, and also catches errors

result types?

what are assertions used for?

type checking
	<type>(value) now returns boolean; redundant
some/none branching
	use the safety operator
catching unbound names
	don't reference unbound names, idiot
bounds checking
	use length types
	instead of going a[2] without knowing if that index is valid
	go list[2](a) to guarantee it
dispatch checking
	signature(<routine>) returns boolean; redundant
catching exceptions
	bad practice; exceptions aren't meant to be caught
	exceptions only kill the task anyway

assertions are unnecessary

---

new error model for sophia

functions can either return successfully or fail to execute
a failed function returns null
null is the value that represents that an exception occurred

if a single failure value is insufficient to represent your failure state,
that's your problem, not the language's
this problem can be solved with proper separation of concerns
any given function should only fail in 1 expected way

for instance, division can return null, but only on division by 0
therefore, the user always knows what exception has occurred

function signatures do not have to declare that they are able to return null
any function is expected to be able to return null

errors represent unexpected, unrecoverable problems
errors terminate their task, but *not* the program as a whole
this allows you to use a supervisor to restart a failed task, should you so choose
like in erlang, tasks are allowed to fail, and you should account for this
errors are severe faults, which incentivises LBYL

---

new parser regime

start with regex matches for statements
then match for expressions inside statements

regex is go
i have to rebuild the entire model of the parser
the parser assumes that the tree is being built bottom-up;
this approach is going top-down
tokens are now instantiated *inside* other token inits
i mean, that was always the case for expressions, wasn't it

statements go
sort out the lexer

type checks currently return any | error
type checks need to return boolean
the rest of the language needs to operate around that assumption

assignments need to be able to see where type checks write to so they can catch a wrong type
there just aren't enough registers to make this work
what to do instead?

special type check that uses the any | error paradigm

type properties are so close to working. i just need a way to conveniently access them
refactor types list as dict

type properties can now be conveniently accessed
unfortunately, type properties still don't compare equal despite having an explicitly defined __eq__
i can't be bothered to figure out why
also i'm pretty sure they won't pickle
so instead i'm going to change the data structure that represents them. again

---

test 18.

change semantics of the bind operator
current behaviour is to call a function and *then* bind it to a future
this requires the parser to be context-sensitive and introduces weird reasoning
proposed behaviour is to bind a function to a future and *then* instantiate the future by calling it
for example:

(a <- x)(1, 2, 3)
or:
a(1, 2, 3) <- x
or:
a <- x{1, 2, 3} // Worse

*or*

x <- (1, 2, 3)

that's better

---

user-defined types!
oh no!

that was way easier than expected. guess the implementation doesn't suck

intersection and union also done. assignment causes no problems at all because types are anonymous

---

function calls have a parsing problem
( is a weird operator
it acts as an infix and takes expressions on both sides
however!
a function call as an entity is expected to behave like the value it returns
so, ( should have minimum bp *inside* the expression but maximum bp *outside* of it
or, minimum bp to its right and maximum bp to its left
which is. impossible?
what even is the desired behaviour here?

there must be some regex trick we can do here

fuck it. just make space separation significant
everything directly adjacent to ( gets parsed as a sub-expression, everything else doesn't
ooh this is gonna be horrible

is it possible to do this for the bit in the brackets too?
is it possible to express the balancing regime in regex?
no. at least not in python re

fixed without using regex. the lexer is powerful; trust its capabilities

so now prefixes bind to the function part of the call instead of the call as a whole
whoops!

fixed again

---

hey. hey. modular functions?
it worked really nicely for types, right?
and you are gonna be composing those things
functions are well-encapsulated so it's not a huge change

proposal:
methods contain an arbitrary number of execution units
when a method is called, each one is executed in sequence. somehow

oh here's a funny idea. queue up task.caller so that returns are automatically redirected to the next unit
hilarious little hack

how do you thread together return values and arguments?
we'll get to that

forget the call stack. time for a call queue
oh man this is busted
the call queue indicates to the runtime the next execution unit to be prepared
when a method returns, the next unit is popped off the queue and the return value is fed into its arguments

this creates the unbelievably fucked-up concept of a 2-dimensional call stack
since execution can progress either up and down (traditional call and return) or sideways (modular execution)

what happens when a function gets called?
and remember that functions and types share the same runtime mechanism

a type check is a function with no subroutines that returns boolean

need to store state in a function and come back to it later? that's what a coroutine is for!

---

execution order got fucked and generators don't pickle.
basic concept is sound but runtime isn't atomic enough to accommodate it elegantly

basic principles

runtime gets a callable instruction
instruction is executed with the instruction's arguments
instruction performs dispatch and returns a callable method
method performs arbitrary actions on the runtime
method returns a value
value is stored in the instruction's address

types are self-contained and can be called anywhere
functions are not
the runtime must be allowed to execute in full in case another function has been called